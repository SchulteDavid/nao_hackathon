# HTWKâ€‘Robotsâ€¯HackathonÂ 2025Â â€“ **Maxâ€‘Stride**â€‘Lokomotionsâ€‘Challenge

Willkommen zur offiziellen Challenge des Reinforcementâ€‘Learningâ€‘Hackathons der HTWKâ€‘Robots!Â Deine Aufgabe ist es, einen zweibeinigen SimulationsÂ­roboter darauf zu trainieren, **mit der grÃ¶ÃŸtmÃ¶glichen durchschnittlichen SchrittlÃ¤nge** vorwÃ¤rts zu laufenÂ â€“ und dabei nicht umzufallen.Â Alles Notwendige ist bereits auf unserem GPUâ€‘Server installiert: Einfach einloggen, die Rewardâ€‘Funktion anpassen und loslegen.

---

## 1Â |Â ReinforcementÂ Learning in 180Â Sekunden

ReinforcementÂ Learning (RL) behandelt Regelung als â€žVersuchÂ &Â Irrtumâ€œ:

| Begriff         | In dieser Challenge                                                                          |
| --------------- | -------------------------------------------------------------------------------------------- |
| **Agent**       | Ein neuronales Netz, das Sollâ€‘Gelenkwinkel fÃ¼r den Roboter ausgibt                           |
| **Umgebung**    | Die Isaacâ€‘Gymâ€‘Physiksimulation (`T1`â€‘Task) liefert Beobachtungen, Reward und ein `done`â€‘Flag |
| **Aktion**      | GewÃ¼nschte Gelenkwinkel (â€“1Â â€¦Â 1) pro SteuerÂ­schritt                                          |
| **Beobachtung** | Gravitation, KÃ¶rperÂ­geschwindigkeiten, GelenkzustÃ¤nde, Gangphase u.Â Befehle                  |
| **Reward**      | Skalar, der jeden Zeitschritt misst, wie gut der Agent ist                                   |
| **Ziel**        | Maximierung der *abgezinsten* Rewardâ€‘SummeÂ â†’ hier: lange Schritte **und** Ãœberleben          |

Das Netz verbessert sich, indem es viele SimulationsÂ­kopien parallel abspielt, Belohnungen sammelt und seine Parameter mit ProximalÂ PolicyÂ Optimisation (PPO) aktualisiert. Nach dem Training kann die Policy exportiert und in Echtzeit ausgefÃ¼hrt werden.

---

## 2Â |Â Deine Mission

> **Trainiere eine Policy, die *************************************************************mittlere SchrittlÃ¤nge************************************************************* Ã¼ber 1â€¯000 SimulationsÂ­schritte maximiert und dabei zufÃ¤llige StÃ¶ÃŸe Ã¼bersteht.**

Du darfst **nur** die Rewardâ€‘Funktion (Terme & Gewichtungen) verÃ¤ndernâ€¯â€“ NetzgrÃ¶ÃŸe, Algorithmus und Hyperparameter sind fix, damit alle die gleichen Startbedingungen haben.Â Das Ranking sortiert Einreichungen nach **durchÂ­schnittlicher SchrittlÃ¤nge (m)**; bei Gleichstand entscheidet **ÃœberlebensÂ­zeit**.

---

## 3Â |Â Schnellstart (ðŸ’¬Â â€žFragÂ Felixâ€œÂ â‰ˆÂ 30â€¯Sekunden)

1. **Webâ€‘Zugang anfordern**Â Â Frag Felix nach den ZugÃ¤ngen fÃ¼r den noVNCâ€‘Client.
2. **Browser Ã¶ffnen**â€¯â†’â€¯du landest auf einem Ubuntuâ€‘Desktop mit vorinstalliertem Isaacâ€¯Gym, PyTorchâ€¯2.2 und VSâ€¯Code.

Danach kannst du sofort in VSâ€¯Code loslegen.

---

## 4Â |Â Repoâ€‘Ãœberblick

```
hackathon-maxstride/
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ T1.yaml          # Umwelt- & Rewardâ€‘Konfiguration
â”‚   â””â”€â”€ t1.py            # Task-Implementierung (erbt BaseTask)
â”œâ”€â”€ train.py             # Lernen der Policy
â”œâ”€â”€ play.py              # AusfÃ¼hren einer trainierten Policy

```

### Wichtige Dateien

| Datei          | Bedeutung                                                              |
| -------------- | ---------------------------------------------------------------------- |
| `tasks/t1.py`  | Definiert Simulation, Beobachtungen **und alle Rewardâ€‘Terme**          |
| `envs/T1.yaml` | EnthÃ¤lt die *Gewichte* jedes Rewardâ€‘Terms.Â WertÂ `0` = Term deaktiviert |
| `train.py`     | Startet das PPOâ€‘Training und speichert Checkpoints & TensorBoardâ€‘Logs  |

---

## 5Â |Â So funktionieren Rewards

In `T1` findest du:

```python
class T1(BaseTask):
    ...
    def _prepare_reward_function(self):
        self.reward_scales = self.cfg["rewards"]["scales"].copy()
        # Terme mit Gewicht 0 werden gestrichen
        for name, scale in self.reward_scales.items():
            self.reward_functions.append(getattr(self, "_reward_" + name))
```

* Jeder Rewardâ€‘Term ist eine Methode, die mit `_reward_` beginnt.
* Ist sein Gewicht in `T1.yaml` **ungleichÂ 0**, wird er pro Schritt ausgefÃ¼hrt und mit diesem Gewicht multipliziert.

Einen neuen Reward hinzufÃ¼genÂ = **Methode schreiben + Gewicht eintragen**.Â Mehr Registrierung braucht es nicht.

---

## 6Â |Â Beispielâ€‘Reward einbauen

1. **Beispielâ€‘Metric**
   Als Demo fÃ¼gen wir einen ultrakurzen Reward hinzu, der einfach die *horizontale VorwÃ¤rtsgeschwindigkeit* belohnt:

   ```python
   # innerhalb von class T1 ---------------------------------
   def _reward_forward_speed(self):
       return self.base_lin_vel[:, 0]   # positive m/s â‡’ positiver Reward
   # --------------------------------------------------------
   ```

2. **Gewicht in ************************************************************`envs/T1.yaml`************************************************************ setzen**

   ```yaml
   rewards:
     scales:
       forward_speed: 1.0   # aktiviert die Metric
   ```

   Wichtig: Jeder neue Reward **muss** hier unter `rewards: scales:` einen Wert erhalten.
   Ein Wert von `0` blendet den Term aus.

---

## 7Â |Â Training & AusfÃ¼hren deiner Policy

### Training starten

```bash
python train.py --task=T1
```

* StandardmÃ¤ÃŸig werden alle Hyperparameter aus `train.py` bzw. `configs/train.yaml` geladen.

* Checkpoints landen unter `logs/<run>/nn/`

### Policy testen

```bash
python play.py --task=T1 --checkpoint=-1   # -1 = letztes Checkpoint
```

Setze `--headless false`, um die Isaacâ€‘Gymâ€‘Vorschau zu sehen.

### Policy exportieren (optional)

```bash
python export_model.py --task=T1 --checkpoint=<iter>
```

Dies erzeugt `model.pt`, falls du die Policy auÃŸerhalb des Hackathons verwenden mÃ¶chtest.
**FÃ¼r die Challenge ist kein Upload nÃ¶tig** â€“ die Bewertung erfolgt lokal auf deinem noVNCâ€‘Desktop.

---

## 8Â |Â TroubleshootingÂ &Â Tipps

* Rewards ausbalancierenÂ â€“ dominiert die Schrittâ€‘Belohnung alles, versucht der Agent evtl. zu springen.

---

Viel ErfolgÂ ðŸš€Â â€“ und mÃ¶ge dein Roboter groÃŸe Schritte machen!

---

Â©Â 2025Â HTWKÂ Robots Â· MITâ€‘Lizenz
